{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fd0fe8-66f2-4c07-8dd6-05e08328a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40248762-40bd-49c9-8744-5a84d9eab1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from modules import NormedConv2d,BcosConv2d,NormedLinear\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc3e722-11e6-41fc-b292-3d16ef6f8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "\tdevice = torch.device(\"mps\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fea7d2e-f63a-439b-9779-84555057d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './processed_female_cells/'\n",
    "files = os.listdir(root_path)\n",
    "files = [i for i in files if not i== '.DS_Store']\n",
    "file_labels = [i.split(' ')[2] for i in files]\n",
    "label_string_to_numreic = {'Mutant':0, 'Control':1, 'Treated':2}\n",
    "labels = [label_string_to_numreic[i] for i in file_labels]\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(files,labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "image_paths_train = []\n",
    "labels_train = []\n",
    "for i,ele in enumerate(train_files):\n",
    "    image_path = os.listdir(root_path+ele)\n",
    "    image_path = [k for k in image_path if k.endswith('.png')]\n",
    "    for k in image_path:\n",
    "        image_paths_train.append(root_path+ele+'/'+k)\n",
    "        labels_train.append(train_labels[i])\n",
    "\n",
    "image_paths_test = []\n",
    "labels_test = []\n",
    "for i,ele in enumerate(test_files):\n",
    "    image_path = os.listdir(root_path+ele)\n",
    "    image_path = [k for k in image_path if k.endswith('.png')]\n",
    "    for k in image_path:\n",
    "        image_paths_test.append(root_path+ele+'/'+k)\n",
    "        labels_test.append(test_labels[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59c0b0f-e420-42fd-aa2b-2b40107fdc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27000,), (27000,), (7500,), (7500,)]\n",
      "(array([0, 1, 2]), array([9000, 9000, 9000])) (array([0, 1, 2]), array([1500, 3000, 3000]))\n"
     ]
    }
   ],
   "source": [
    "print ([np.shape(i) for i in [image_paths_train, labels_train, image_paths_test, labels_test]])\n",
    "print (np.unique(labels_train,return_counts=True),np.unique(labels_test,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff34d08-cda6-4c86-8958-7c92994448b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0202a10e-cfbd-404d-b02e-5ecb7c60540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/helium/.cache/torch/hub/B-cos_B-cos-v2_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('B-cos/B-cos-v2', 'vitc_l_patch1_14', pretrained=True)\n",
    "model[0].linear_head.linear.linear = NormedLinear(1024,2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04523645-8708-46a9-abd9-d46c151fc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = model.transform #transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CustomDataset(image_paths_train, labels_train, transform=transform)\n",
    "test_dataset = CustomDataset(image_paths_test, labels_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1765389c-2741-43a0-bb62-6f3b3eb4b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print (images.shape,labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e2f7bd-975d-48a8-82c6-4bd642303e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4cf75a-8b88-4d4b-8e9b-90b86969144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('./model_2.pth',map_location=torch.device(device))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9ce641-4b1c-4416-aebf-d7b36f85716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 235/235 [03:15<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, data_loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)  # labels are now integers\n",
    "            outputs = model(inputs)\n",
    "            # No activation needed; directly take the argmax of the logits\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total * 100.0\n",
    "    return accuracy\n",
    "print (calculate_accuracy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10415867-32e1-4222-9a11-142330597c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "test_log = 0\n",
    "log = []\n",
    "counter = 0\n",
    "for epoch in range(epochs):\n",
    "    counter +=1\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        #outputs = sigmoid(outputs.to(device))\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs.to(device), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    test_acc = calculate_accuracy(model,test_loader)\n",
    "    print (test_acc)\n",
    "    log.append(test_acc)\n",
    "    if test_acc>test_log:\n",
    "        counter = 0\n",
    "        test_log = test_acc\n",
    "        #torch.save(model.state_dict(), './model_2.pth')\n",
    "    if counter > 40:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1850fae-c423-4158-8731-c2d0b0d8a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = calculate_accuracy(model,test_loader)\n",
    "print (test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a313ac46-f79c-4336-9d50-07b184cdea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_explanation(pre_processed_image, feature_num, model=model, device=device):\n",
    "    # Move the input tensor to the same device as the model\n",
    "    input_tensor = pre_processed_image[None].to(device).requires_grad_()\n",
    "    with model.explanation_mode():\n",
    "        out = model(input_tensor)\n",
    "        indices = torch.tensor([feature_num]).to(device)\n",
    "        prediction_logit = torch.gather(out, 1, indices.unsqueeze(1)).squeeze(1)\n",
    "        prediction_logit.backward(inputs=[input_tensor])\n",
    "    dynamic_linear_weights = input_tensor.grad\n",
    "    explanation = model.gradient_to_image(input_tensor[0], dynamic_linear_weights[0])\n",
    "    return explanation\n",
    "\n",
    "def get_saliency_of_explanation(pre_processed_image,feature_num,model=model,tr_power = 1):\n",
    "    sal = get_feature_explanation(pre_processed_image,feature_num,model)[:,:,3]\n",
    "    sal = np.power(sal, tr_power)\n",
    "    return sal\n",
    "\n",
    "def preprocessed_to_display_image(img):\n",
    "    return img[:3,:,:].permute(1,2,0).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80be03dd-f325-482b-b2aa-5876c57e451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith('.png')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path  # return the path for identification if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36ee9674-1d4d-432f-89e5-de0ef5ce6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_dir = './images_eval/'\n",
    "roi_dataset = ROIDataset(roi_dir, transform=model.transform)\n",
    "roi_loader = DataLoader(roi_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05969fb8-d38a-4344-8f9d-be761eb7204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: ./images_eval/S1480 Ov Mutant Image 1.png - Predicted Class: 1\n",
      "Image: ./images_eval/S1478 Ov Mutant.png - Predicted Class: 1\n",
      "Image: ./images_eval/L4567 Ov Control.png - Predicted Class: 1\n",
      "Image: ./images_eval/S1479 Ov Control image 3.png - Predicted Class: 1\n",
      "Image: ./images_eval/L3505 Ov Control.png - Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in roi_loader:\n",
    "        images = images.to(device)  # Move images to the same device as the model\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for path, pred in zip(paths, preds):\n",
    "            predictions[path] = pred.item()\n",
    "\n",
    "for img_path, pred in predictions.items():\n",
    "    print(f\"Image: {img_path} - Predicted Class: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82921af5-3d67-43fa-af6c-e450443f2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = os.listdir('./images_eval/')\n",
    "animals = [i for i in animals if i != '.DS_Store']\n",
    "c = 0\n",
    "\n",
    "for im, paths in roi_loader:\n",
    "    image = Image.open(paths[0])\n",
    "    im = im.to(device)\n",
    "    outputs = model(im)\n",
    "    \n",
    "    s1 = get_saliency_of_explanation(im[0], 0)\n",
    "    s2 = get_saliency_of_explanation(im[0], 1)\n",
    "    plot_im = preprocessed_to_display_image(im[0])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    plt.savefig(f'saved_outputs/{c}_original.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    " #   ax.imshow(image)\n",
    "    ax.imshow(s1, cmap='jet')\n",
    "    ax.axis('off')\n",
    "    plt.savefig(f'saved_outputs/{c}_saliency1.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "#    ax.imshow(image)\n",
    "    ax.imshow(s2, cmap='jet')\n",
    "    ax.axis('off')\n",
    "    plt.savefig(f'saved_outputs/{c}_saliency2.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9685354-3a14-4b24-9fe2-bae338c4bdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
